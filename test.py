import torch
from PIL import Image
import sys
import os
from pathlib import Path

# ç›´æ¥æ·»åŠ æ¨¡å‹è·¯å¾„åˆ°ç³»ç»Ÿè·¯å¾„
model_path = r"D:\work\AUTO1111\webui\models\Zero123\zero123plus-v1.1"
sys.path.insert(0, model_path)

# ç›´æ¥å¯¼å…¥æ¨ç†ä»£ç 
inference_file = os.path.join(model_path, "inference.py")
if not os.path.exists(inference_file):
    print(f"âŒ æ¨ç†æ–‡ä»¶ä¸å­˜åœ¨: {inference_file}")
    sys.exit(1)

# åŠ¨æ€å¯¼å…¥æ¨ç†æ¨¡å—
import importlib.util
spec = importlib.util.spec_from_file_location("inference", inference_file)
inference = importlib.util.module_from_spec(spec)
spec.loader.exec_module(inference)

print("âœ… æˆåŠŸå¯¼å…¥æ¨ç†ä»£ç ")

def test_direct_inference():
    """ç›´æ¥ä½¿ç”¨åŸå§‹æ¨ç†ä»£ç æµ‹è¯•"""
    print("ğŸ¯ ç›´æ¥æ¨ç†æµ‹è¯•...")
    
    try:
        # åŠ è½½å›¾åƒ
        test_image_path = r"D:\work\AUTO1111\webui\1.png"
        if not os.path.exists(test_image_path):
            print("âŒ æµ‹è¯•å›¾åƒä¸å­˜åœ¨")
            return False
            
        image = Image.open(test_image_path).convert("RGB")
        image = image.resize((256, 256))
        print(f"è¾“å…¥å›¾åƒ: {image.size}")
        
        # ç›´æ¥ä½¿ç”¨åŸå§‹æ¨ç†ä»£ç 
        print("ä½¿ç”¨åŸå§‹ Zero123PlusPipeline...")
        
        # ä»æ¨¡å‹ç›®å½•åŠ è½½ç®¡é“
        pipeline = inference.Zero123PlusPipeline.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            local_files_only=True
        )
        
        if torch.cuda.is_available():
            pipeline = pipeline.to("cuda")
            print("âœ… ä½¿ç”¨ GPU")
        
        # å‡†å¤‡ç®¡é“ï¼ˆè°ƒç”¨ prepare æ–¹æ³•ï¼‰
        if hasattr(pipeline, 'prepare'):
            pipeline.prepare()
            print("âœ… ç®¡é“å‡†å¤‡å®Œæˆ")
        
        # ç”Ÿæˆæµ‹è¯•
        print("ç”Ÿæˆå³ä¾§è§†è§’...")
        result = pipeline(
            image=image,
            elevation=0,
            azimuth=90,
            num_inference_steps=15,  # è¾ƒå°‘çš„æ­¥æ•°ç”¨äºæµ‹è¯•
            guidance_scale=3.0,
            height=256,
            width=256,
            output_type="pil"
        )
        
        # ä¿å­˜ç»“æœ
        output_path = "direct_test_output.jpg"
        if hasattr(result, 'images') and result.images:
            result.images[0].save(output_path)
            print(f"âœ… ç”Ÿæˆå®Œæˆ: {output_path}")
            return True
        else:
            print("âŒ æ²¡æœ‰ç”Ÿæˆå›¾åƒ")
            return False
            
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_simple_components():
    """ç®€å•ç»„ä»¶æµ‹è¯•"""
    print("\nğŸ”§ ç®€å•ç»„ä»¶æµ‹è¯•...")
    
    try:
        # æµ‹è¯•ç›´æ¥åŠ è½½ç»„ä»¶
        from transformers import CLIPTextModel, CLIPTokenizer, CLIPImageProcessor, CLIPVisionModelWithProjection
        from diffusers import AutoencoderKL, UNet2DConditionModel, EulerAncestralDiscreteScheduler
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        dtype = torch.float16 if torch.cuda.is_available() else torch.float32
        
        print("1. æµ‹è¯• VAE åŠ è½½...")
        vae = AutoencoderKL.from_pretrained(
            os.path.join(model_path, "vae"),
            local_files_only=True
        ).to(device, dtype=dtype)
        print("   âœ… VAE åŠ è½½æˆåŠŸ")
        
        print("2. æµ‹è¯• UNet åŠ è½½...")
        unet = UNet2DConditionModel.from_pretrained(
            os.path.join(model_path, "unet"),
            local_files_only=True
        ).to(device, dtype=dtype)
        print("   âœ… UNet åŠ è½½æˆåŠŸ")
        
        print("3. æµ‹è¯•æ–‡æœ¬ç¼–ç å™¨åŠ è½½...")
        text_encoder = CLIPTextModel.from_pretrained(
            os.path.join(model_path, "text_encoder"),
            local_files_only=True
        ).to(device, dtype=dtype)
        print("   âœ… æ–‡æœ¬ç¼–ç å™¨åŠ è½½æˆåŠŸ")
        
        print("4. æµ‹è¯•è§†è§‰ç¼–ç å™¨åŠ è½½...")
        vision_encoder = CLIPVisionModelWithProjection.from_pretrained(
            os.path.join(model_path, "vision_encoder"),
            local_files_only=True
        ).to(device, dtype=dtype)
        print("   âœ… è§†è§‰ç¼–ç å™¨åŠ è½½æˆåŠŸ")
        
        print("ğŸ‰ æ‰€æœ‰ç»„ä»¶åŠ è½½æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ ç»„ä»¶æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸª Zero-1-to-3 ç›´æ¥æµ‹è¯•å·¥å…·")
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    
    # æ£€æŸ¥æ¨¡å‹ç›®å½•
    if not os.path.exists(model_path):
        print("âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨")
        return
    
    # 1. å…ˆæµ‹è¯•ç»„ä»¶åŠ è½½
    print("\n=== é˜¶æ®µ1: ç»„ä»¶åŠ è½½æµ‹è¯• ===")
    if not test_simple_components():
        print("ğŸ’¥ ç»„ä»¶åŠ è½½æµ‹è¯•å¤±è´¥")
        return
    
    # 2. æµ‹è¯•å®Œæ•´æ¨ç†
    print("\n=== é˜¶æ®µ2: å®Œæ•´æ¨ç†æµ‹è¯• ===")
    if test_direct_inference():
        print("ğŸ‰ å®Œæ•´æµ‹è¯•æˆåŠŸï¼")
    else:
        print("ğŸ’¥ æ¨ç†æµ‹è¯•å¤±è´¥")

if __name__ == "__main__":
    main()